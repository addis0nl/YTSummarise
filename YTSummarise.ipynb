{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "jHK0Qn8p5ekq"
      ],
      "authorship_tag": "ABX9TyMJU3r39Kuan/HxNzWZIzC6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/addis0nl/YTSummarise/blob/main/YTSummarise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Summarise YouTube videos w/ LLM via built-in transcriptions\n",
        "\n",
        "This colab notebook uses 13b model instead of 7b.\n",
        "\n",
        "Make sure to select a GPU runtime."
      ],
      "metadata": {
        "id": "Emx7KVvj_GzX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "jHK0Qn8p5ekq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install youtube_transcript_api\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python\n",
        "!wget https://huggingface.co/TheBloke/LlongOrca-13B-16K-GGML/resolve/main/llongorca-13b-16k.ggmlv3.q4_K_S.bin\n",
        "\n",
        "from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled\n",
        "from llama_cpp import Llama\n",
        "\n",
        "# Change function to suit model if neccessary\n",
        "def get_path():\n",
        "  return \"llongorca-13b-16k.ggmlv3.q4_K_S.bin\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-OPP6Zn3iyx",
        "outputId": "da1307ec-07e4-4562-e85a-6041e7444140"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting youtube_transcript_api\n",
            "  Downloading youtube_transcript_api-0.6.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from youtube_transcript_api) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->youtube_transcript_api) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->youtube_transcript_api) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->youtube_transcript_api) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->youtube_transcript_api) (2023.7.22)\n",
            "Installing collected packages: youtube_transcript_api\n",
            "Successfully installed youtube_transcript_api-0.6.1\n",
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.1.78.tar.gz (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.7.1)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.23.5)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.6/45.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.1.78-cp310-cp310-linux_x86_64.whl size=5822262 sha256=98e63520e88252548fa5a32773c53a75d7bc1c24aabf1a616b0846019c788203\n",
            "  Stored in directory: /root/.cache/pip/wheels/61/f9/20/9ca660a9d3f2a47e44217059409478865948b5c8a1cba70030\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.1 llama-cpp-python-0.1.78\n",
            "--2023-08-25 09:04:57--  https://huggingface.co/TheBloke/LlongOrca-13B-16K-GGML/resolve/main/llongorca-13b-16k.ggmlv3.q4_K_S.bin\n",
            "Resolving huggingface.co (huggingface.co)... 65.9.86.57, 65.9.86.79, 65.9.86.62, ...\n",
            "Connecting to huggingface.co (huggingface.co)|65.9.86.57|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/4d/ef/4defa3c730af977fc96ebb954bc7da9fb5576ccb491c17ab70ee469c8c9a264c/a719c28f4ad57b2ffb68e94d74bb2706ed7ce83a8b2c28694b392327142ac6e6?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27llongorca-13b-16k.ggmlv3.q4_K_S.bin%3B+filename%3D%22llongorca-13b-16k.ggmlv3.q4_K_S.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1693213497&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY5MzIxMzQ5N319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy80ZC9lZi80ZGVmYTNjNzMwYWY5NzdmYzk2ZWJiOTU0YmM3ZGE5ZmI1NTc2Y2NiNDkxYzE3YWI3MGVlNDY5YzhjOWEyNjRjL2E3MTljMjhmNGFkNTdiMmZmYjY4ZTk0ZDc0YmIyNzA2ZWQ3Y2U4M2E4YjJjMjg2OTRiMzkyMzI3MTQyYWM2ZTY%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=AEjjeUYRBe5kcXE0Gt9pRiDqN03QOFiblm%7Ed4NVehSpGhjebCq9FBsu88rFcwXJrFKFsjEoWwFUPbh6tRCJzPO185ow%7ExbU4gKLjJDwGMg-DHjUD%7EuRsM-QATVWcuS9R06eMdeDT5JDoobewJpygDAqZtfk-MKZny92CuoWN8vplgjnZTnGS4v6Q54h65oUwBwwsXUypAwOUDzEcNGjdTUsgqjypLbRjY8J1luFLsUXcruQyvL8qJHioiyk60ZlLQVulixFGD-7BtqhEQWvtynKZM9DqgXU7rZ8LxQnWVljAuZ6Ya38ReoKr5wqH2nAvQPYHm8Md-qMgCoRIL4fVww__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2023-08-25 09:04:57--  https://cdn-lfs.huggingface.co/repos/4d/ef/4defa3c730af977fc96ebb954bc7da9fb5576ccb491c17ab70ee469c8c9a264c/a719c28f4ad57b2ffb68e94d74bb2706ed7ce83a8b2c28694b392327142ac6e6?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27llongorca-13b-16k.ggmlv3.q4_K_S.bin%3B+filename%3D%22llongorca-13b-16k.ggmlv3.q4_K_S.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1693213497&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY5MzIxMzQ5N319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy80ZC9lZi80ZGVmYTNjNzMwYWY5NzdmYzk2ZWJiOTU0YmM3ZGE5ZmI1NTc2Y2NiNDkxYzE3YWI3MGVlNDY5YzhjOWEyNjRjL2E3MTljMjhmNGFkNTdiMmZmYjY4ZTk0ZDc0YmIyNzA2ZWQ3Y2U4M2E4YjJjMjg2OTRiMzkyMzI3MTQyYWM2ZTY%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=AEjjeUYRBe5kcXE0Gt9pRiDqN03QOFiblm%7Ed4NVehSpGhjebCq9FBsu88rFcwXJrFKFsjEoWwFUPbh6tRCJzPO185ow%7ExbU4gKLjJDwGMg-DHjUD%7EuRsM-QATVWcuS9R06eMdeDT5JDoobewJpygDAqZtfk-MKZny92CuoWN8vplgjnZTnGS4v6Q54h65oUwBwwsXUypAwOUDzEcNGjdTUsgqjypLbRjY8J1luFLsUXcruQyvL8qJHioiyk60ZlLQVulixFGD-7BtqhEQWvtynKZM9DqgXU7rZ8LxQnWVljAuZ6Ya38ReoKr5wqH2nAvQPYHm8Md-qMgCoRIL4fVww__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 18.239.94.36, 18.239.94.23, 18.239.94.6, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|18.239.94.36|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7558877632 (7.0G) [application/octet-stream]\n",
            "Saving to: ‘llongorca-13b-16k.ggmlv3.q4_K_S.bin’\n",
            "\n",
            "llongorca-13b-16k.g 100%[===================>]   7.04G  15.7MB/s    in 7m 13s  \n",
            "\n",
            "2023-08-25 09:12:11 (16.6 MB/s) - ‘llongorca-13b-16k.ggmlv3.q4_K_S.bin’ saved [7558877632/7558877632]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sysprompt():\n",
        "    return \"### System: You are an AI assistant that summarises videos based on it's audio transcript. You only use information and context from the transcript, in other words, you do not state anything that is not explicitly stated in the transcript.\\n\\n\"\n",
        "\n",
        "def quick_llm(text):\n",
        "    llama = Llama(model_path=get_path(), n_gpu_layers=-1, seed=-1, n_ctx=4096, rope_freq_scale=0.5)\n",
        "    if detailed:\n",
        "        for sections in text:\n",
        "            section = ''.join(sections)\n",
        "            output = llama(sysprompt()+'### Instruction:\\n\\nList the details from the video shown here:\\n\"' +section +\n",
        "                           '\"\\n\\n### Response:\\n', max_tokens=2048, temperature=0.5, stream=stream)\n",
        "            display(output)\n",
        "    else:\n",
        "        output = llama(sysprompt()+'### Instruction:\\n\\nSummarise the following concisely:\\n\"' + text + '\"\\n\\n### Response:\\n',\n",
        "                     max_tokens=2048, stop=['###'], stream=stream, temperature=0.4)\n",
        "        display(output)\n",
        "\n",
        "def llm(text):\n",
        "    llama = Llama(model_path=get_path(), n_gpu_layers=40, seed=-1, n_ctx=16384, rope_freq_scale=0.25)\n",
        "    output = llama(sysprompt()+'### Instruction:\\n\\nAccurately summarise all important information from the following:\\n\"'\n",
        "                   + text + '\"\\n\\n### Response:\\n', max_tokens=4096, stop=['###'], stream=stream, temperature=0.3)\n",
        "    display(output)\n",
        "\n",
        "def out(text):\n",
        "    outfile = \"output/\" + video_id + \".txt\"\n",
        "    with open(outfile, \"a\") as f:\n",
        "        f.write(text+\"\\n\")\n",
        "\n",
        "def display(output):\n",
        "    if stream:\n",
        "        for x in output:\n",
        "            print(x['choices'][0]['text'], end=\"\")\n",
        "    else:\n",
        "        print(output['choices'][0]['text'], end=\"\")\n",
        "        if out:\n",
        "            out(output['choices'][0]['text'])\n",
        "\n",
        "def get_transcript(id):\n",
        "    try:\n",
        "        transcript = YouTubeTranscriptApi.get_transcript(id)\n",
        "    except TranscriptsDisabled:\n",
        "        return None\n",
        "    length = transcript[-1]['start'] + transcript[-1]['duration']\n",
        "    n_segments = int(length / 60 / 5)\n",
        "    segments = [[] for i in range(n_segments + 1)]\n",
        "    for line in transcript:\n",
        "        segments[int(line['start'] / 60 / 5)].append(line['text'] + \" \")\n",
        "    return segments\n",
        "\n",
        "def combine_sub(segments):\n",
        "    return ''.join([''.join(s) for s in segments])\n",
        "\n",
        "def summary(segments):\n",
        "    all_text = combine_sub(segments)\n",
        "    if long_video:\n",
        "        mid = len(segments)//2\n",
        "        a = combine_sub(segments[:mid])\n",
        "        b = combine_sub(segments[mid:])\n",
        "        print(\"\\n\\nPart 1 of 2:\\n\\n\")\n",
        "        llm(a)\n",
        "        print(\"\\n\\nPart 2 of 2:\\n\\n\")\n",
        "        llm(b)\n",
        "    elif detailed:\n",
        "        quick_llm(segments)\n",
        "    else:\n",
        "        all_text = combine_sub(segments)\n",
        "        if quick:\n",
        "            quick_llm(all_text)\n",
        "        else:\n",
        "            llm(all_text)"
      ],
      "metadata": {
        "id": "YIl_kdS35Tuy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "5DvEVqUf-_cY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**To Use:**\n",
        "\n",
        "1. In the ID field below, enter the alphanumeric id of your chosen YouTube video, i.e. youtu.be/[id] or youtube.com/watch?v=[id]\n",
        "2. Choose optional parameters\n",
        "3. Run cell and again if necessary\n",
        "\n",
        "**Optional Parameters:**\n",
        "\n",
        "* *stream* - Output text as soon as it's created\n",
        "* *quick* - Use smaller context LLM for faster inference\n",
        "* *detailed* - Summarise every 5 minute chunk of the video\n",
        "* *long_video* - Use larger context LLM for longer videos (experimental)\n",
        "* *out* - Also output into .txt file, can be used with transcript option\n",
        "* *transcript* - Output full transcript only, no summarisation"
      ],
      "metadata": {
        "id": "SiJi-u22CWha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ID = \"dQw4w9WgXcQ\" # @param {type:\"string\"}\n",
        "\n",
        "stream = False # @param {type:\"boolean\"}\n",
        "quick = False # @param {type:\"boolean\"}\n",
        "detailed = False # @param {type:\"boolean\"}\n",
        "long_video = False # @param {type:\"boolean\"}\n",
        "out = False # @param {type:\"boolean\"}\n",
        "transcript = False # @param {type:\"boolean\"}\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    video_id = ID\n",
        "    subs = get_transcript(video_id)\n",
        "    if transcript:\n",
        "        transcript = combine_sub(subs)\n",
        "        print(transcript)\n",
        "        if out:\n",
        "            out(transcript)\n",
        "    else:\n",
        "        summary(subs)"
      ],
      "metadata": {
        "id": "ZfbeqczF8Ygr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64a24288-f7c6-4cbe-93c1-8bb9f9317434"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The video features a song with the lyrics, \"Thank you we're no strangers to love.\" The singer expresses their feelings and wants to make the listener understand. They mention not being like any other guy and promise never to let the listener down."
          ]
        }
      ]
    }
  ]
}